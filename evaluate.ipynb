{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/autoresponder/lib/python3.10/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'ground_truth', 'input_ids', 'attention_mask', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "full_ds = load_from_disk(\"data/chat_history\")\n",
    "\n",
    "# We only want to evaluate on the test set to ensure we didn't overfit\n",
    "sample_data = full_ds[\"test\"].to_pandas().rename(columns={\"text\": \"ground_truth\"})\n",
    "\n",
    "sample_data = sample_data.sample(n=100, random_state=1)\n",
    "\n",
    "print(len(sample_data))\n",
    "sample_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from rich.progress import track\n",
    "\n",
    "\n",
    "def create_inference_function(model):\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=\"google/flan-t5-base\",\n",
    "        device=0,\n",
    "    )\n",
    "\n",
    "    def run_inference(prompts, **kwargs):\n",
    "        # pipelines appear to only return iterators when used with a KeyDataset, and we have to create a Dataset to create a KeyDataset\n",
    "        prompts_ds = Dataset.from_dict({\"prompt\": prompts})\n",
    "        prompts_ds = KeyDataset(prompts_ds, \"prompt\")\n",
    "\n",
    "        all_predictions = pipe(\n",
    "            prompts_ds,\n",
    "            max_length=200,\n",
    "            num_beams=5,\n",
    "            repetition_penalty=3.0,\n",
    "            num_return_sequences=5,\n",
    "            batch_size=6,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for predictions in all_predictions:\n",
    "            yield [p[\"generated_text\"] for p in predictions]\n",
    "\n",
    "    return run_inference\n",
    "\n",
    "\n",
    "# test_fn = create_inference_function(\n",
    "#     AutoModelForSeq2SeqLM.from_pretrained(f\"google/flan-t5-base\")\n",
    "# )\n",
    "# test_output = list(track(\n",
    "#     test_fn(test_ds[\"prompt\"].tolist(), batch_size=16), total=len(test_ds[\"prompt\"])\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = {\n",
    "    # \"t5-lg\": AutoModelForSeq2SeqLM.from_pretrained(f\"google/flan-t5-large\"),\n",
    "    \"t5-lg-29000\": AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        f\"models/flan-t5-large-telegram/checkpoint-29000\"\n",
    "    ),\n",
    "    \"t5-lg-42000\": AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        f\"models/flan-t5-large-10ep/checkpoint-42000\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "models_to_compare = {\n",
    "    k: create_inference_function(v) for k, v in models_to_compare.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from diskcache import Cache\n",
    "import time\n",
    "\n",
    "# Load the OpenAI API key from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "\n",
    "cache = Cache(\"data/openai_cache\")\n",
    "\n",
    "\n",
    "@cache.memoize(\"oai_infer_dv3_n5_mt200\")\n",
    "def openai_inference(prompt):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, n=5, max_tokens=200\n",
    "    )\n",
    "\n",
    "    return [completion[\"text\"] for completion in completions[\"choices\"]]\n",
    "\n",
    "\n",
    "def openai_batch_inference(prompts):\n",
    "    for prompt in prompts:\n",
    "        yield openai_inference(prompt)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "openai_inference(\"a man a plan a canal\")\n",
    "\n",
    "models_to_compare[\"openai\"] = openai_batch_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f1de501b0245c1baa8cc6daebc1f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.progress import Progress, MofNCompleteColumn\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prompts = list(sample_data[\"prompt\"])\n",
    "\n",
    "with Progress(*Progress.get_default_columns(), MofNCompleteColumn()) as progress:\n",
    "    for model_name, inference_fn in models_to_compare.items():\n",
    "        # if model_name != 't5-lg-29000':\n",
    "        #     continue\n",
    "        predictions[model_name] = list(\n",
    "            progress.track(\n",
    "                inference_fn(prompts), description=model_name, total=len(prompts)\n",
    "            )\n",
    "        )\n",
    "\n",
    "sample_data = sample_data.assign(**predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        table {\n",
       "            /* Make all columns the same width */\n",
       "            table-layout: fixed;\n",
       "            background-color: #f5f5f5;\n",
       "            \n",
       "            /* break long urls */\n",
       "            word-wrap: break-word;\n",
       "            word-break: break-word;\n",
       "        }\n",
       "        th, td {\n",
       "            max-width: 300px;\n",
       "            text-align: left;\n",
       "            vertical-align: top;\n",
       "            font-family: sans-serif;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prompt</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>t5-lg-29000</th>\n",
       "      <th>t5-lg-42000</th>\n",
       "      <th>openai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Respond to chat as Kyle Corbitt:<br><br>Recent messages:<br><br>Jacqueline Corbitt:<br>Vanessa's cousin Mikey wants everyone to know that his T-Rex is bigger than Vanessa üòÅ<br><br>David Corbitt:<br>Lucky for him she can't walk yet<br><br>David Corbitt:<br>Merry Christmas<br><br>Scott Corbitt:<br>Shoulda been Bon Nadal my bad<br><br>Scott Corbitt:<br>Hey everybody, you keep promising to post pictures. Let's make that happen (so I can repost some of them to Instagram)<br><br>Christina Corbitt:<br>Merry Christmas üéÑ<br><br>Jacqueline Corbitt:<br>Baby's first stocking<br><br>Jacqueline Corbitt:<br>Vanessa loves her Uncle Kevin, much to the jealousy of Uncle Kevin's own child üòÇ<br><br>Christina Corbitt:<br>We have a white Christmas in Washington üòå<br><br>Karen Corbitt:<br>Oh my.  Are the roads ok?<br><br>Kyle Corbitt:<br></td>\n",
       "      <td>Wow nice!!</td>\n",
       "      <td>Yeah they're ok<br>--------------------<br>yep, they're pretty good<br>--------------------<br>yep, they're all good<br>--------------------<br>yep<br>--------------------<br>yep!</td>\n",
       "      <td>Yeah they're pretty ok here<br>--------------------<br>Yeah they're pretty ok right now<br>--------------------<br>Yeah they're ok<br>--------------------<br>Yeah they're pretty ok<br>--------------------<br>Yeah they're all good here</td>\n",
       "      <td>Merry Christmas everyone! And Karen, I heard the roads should be okay, but drive safely!<br>--------------------<br>Hey everyone! Wow, what a great conversation! I'm sure Mikey's T-Rex is huge! And yeah, let's post some pics so the rest of the family can see what we're up to. It sounds like Vanessa already has quite the fan club! As for the snow out in Washington, Karen, I'm sure it looks beautiful, but please be careful if you're out on the roads! ü§ó<br>--------------------<br>Ha! That's funny, Jacqueline. Merry Christmas, everyone! Let's see those pictures! Karen, yes, the roads have been cleared.<br>--------------------<br>Merry Christmas to all! It sounds like everyone is having a great holiday. And yes Karen, the roads in Washington should be OK - mostly just snowing here in New York.<br>--------------------<br>Hey everyone! Haha, I love the stories about Vanessa and Uncle Kevin. Definitely wanna see those photos! Karen, roads here in Phoenix are fine. Wish you all a very Merry Christmas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Respond to chat as Kyle Corbitt:<br><br>Recent messages:<br><br>David Corbitt:<br>I am really excited to try this. They say 46 minutes of flight time but if it works for 30 I'll be happy<br><br>Kyle Corbitt:<br>nice<br><br>Kyle Corbitt:<br></td>\n",
       "      <td>been thinking about buying a drone forever</td>\n",
       "      <td>if it works for 30 I'll be happy<br>--------------------<br>if it works for 30 I'd be happy<br>--------------------<br>if it works for 30 that's great<br>--------------------<br>if it works for 30 I'm happy<br>--------------------<br>what's the price?</td>\n",
       "      <td>if it works for 30 that's good enough for me<br>--------------------<br>if it works for 30 that's great<br>--------------------<br>if it works for 30 that's good enough for me!<br>--------------------<br>if it works for 30 I'm happy<br>--------------------<br>if it works for 30 minutes that's good enough for me</td>\n",
       "      <td>Sounds great, David! I'm excited to hear how it works out for you!<br>--------------------<br>I'm excited to try this too! It sounds like a fun experience. Let's hope it works for at least the full 46 minutes!<br>--------------------<br>That sounds great, David. You should definitely give it a try and let us know how it goes!<br>--------------------<br>That sounds great, David! I'm looking forward to testing it out -- let's aim for at least 30 minutes of flight time and see how it goes.<br>--------------------<br>That sounds great! I'm sure you'll be able to get at least 30 minutes out of it. Good luck with your flight!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pprint\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sample_formatted = sample_data.copy()[[\"prompt\", \"ground_truth\", *predictions.keys()]]\n",
    "\n",
    "\n",
    "for col in predictions.keys():\n",
    "    sample_formatted[col] = sample_formatted[col].apply(\n",
    "        lambda x: f\"\\n{'-'*20}\\n\".join(x)\n",
    "    )\n",
    "\n",
    "def df_to_html(df):\n",
    "    table_html = df.to_html(index=False, escape=False).replace(\"\\\\n\", \"<br>\")\n",
    "    \n",
    "    table_styling = \"\"\"\n",
    "    <style>\n",
    "        table {\n",
    "            /* Make all columns the same width */\n",
    "            table-layout: fixed;\n",
    "            background-color: #f5f5f5;\n",
    "            \n",
    "            /* break long urls */\n",
    "            word-wrap: break-word;\n",
    "            word-break: break-word;\n",
    "        }\n",
    "        th, td {\n",
    "            max-width: 300px;\n",
    "            text-align: left;\n",
    "            vertical-align: top;\n",
    "            font-family: sans-serif;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    return table_styling + table_html\n",
    "\n",
    "display(HTML(df_to_html(sample_formatted.head(2))))\n",
    "\n",
    "with open(\"data/table.html\", \"w\") as f:\n",
    "    f.write(df_to_html(sample_formatted))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoresponder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767c9e6bcb6bf1e56b6e269cd77223d5a15d6d7a64e3ff78b92463e57e0451c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
