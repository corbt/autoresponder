This repository contains the code I used to successfully fine-tune [FLAN T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) on my personal Telegram chat history. You can see some sample outputs from the final model [here](https://htmlpreview.github.io/?https://github.com/corbt/autoresponder/blob/main/results.html). The results from my models are in the columns labeled "Fine-tuned FLAN T5 XL" and "Fine-tuned FLAN T5 Large".

## Why?

I wanted to see whether I could create an smart typeahead bot for Telegram, similar to how Copilot works in VS Code.

## Results

The most capable T5-Large model I trained was fine-tuned for 5 epochs on a Lambda Labs A100. This took 6 hours for a total cost of ~$7. The most capable T5-XL model I trained was fine-tuned for 5 epochs on an 80GB A100 at [RunPod](https://www.runpod.io/). This took 16 hours for a total cost of ~$34. You can find the exact code used in training in [./train-t5-large.py](./train-t5-large.py) and [./train-t5-xl.py](./train-t5-xl.py).

You can view some sample inference results for both models side-by-side [here](https://htmlpreview.github.io/?https://github.com/corbt/autoresponder/blob/main/results.html). Both models performed significantly better than their non-fine-tuned T5 base models. With low-latency inference I could imagine either model being useful as a typeahead assistant. For this use case and dataset the T5-XL model with 3B parameters doesn't seem to significantly outperform the T5-Large model with 770M parameters, so the smaller model is probably the way to go for faster and cheaper inference.

## Technical details

### Data

I exported my chat history from 4 long-running Telegram chats (with my wife, brother, family and co-founder). There were 85K messages total, including 32K messages from me. For each message I formatted the previous several messages from the same conversation as the prompt, and used the message itself as the target output. I split this dataset into the oldest 98% of messages as the training set, and the most recent 2% of messages as the validation set. You can see all my data prep code as well as some aggregate stats on the dataset at [./prep-data.ipynb](./prep-data.ipynb).

### Training

I used this dataset to fine-tune two models: [FLAN-T5 Large](https://huggingface.co/google/flan-t5-large) and [FLAN-T5 XL](https://huggingface.co/google/flan-t5-xl). As of early 2023 when I performed this experiment FLAN-T5 is the most capable pretrained seq2seq model available (I expect this to change quickly!). I used the Adam optimizer with learning rates of 3e-4, 1e-4, 1e-5 and 1e-6 and various numbers of epochs from 3 to 20. I trained the Large models on a 40GB A100 GPU I rented from [Lambda Labs](https://lambdalabs.com/), and the XL models on an 80GB A100 GPU I rented from [RunPod](https://www.runpod.io/).

You can find my training scripts in [./train-t5-large.py](./train-t5-large.py) and [./train-t5-xl.py](./train-t5-xl.py).

### Evaluation

For evaluation I randomly selected 100 chat histories from the test set and generated 5 candidate completions for each of them across a variety of models, including my fine-tuned T5 variants, the base T5 variants and (as a point of comparison) GPT-3. I then subjectively compared the quality of the generated responses across all of them. You can find my evaluation script at [./evaluate.ipynb](./evaluate.ipynb).

I don't intend to publish any model weights for this project to avoid leaking details about private conversations, sorry!

### Inference

I attempted to load my model into [Triton inference server](https://github.com/triton-inference-server/server), which should be capable of producing thousands of tokens a second for T5-Large. However, the script required to translate the weights from the HuggingFace Transformers model to a FasterTransformer model didn't work off-the-shelf for me, and optimizing inference speed wasn't a priority for this project so I aborted.

I did set up a [Streamlit](./streamlit-inference.py) inference app to let me quickly paste different conversations and see what the bot would have come up with.

## Repository Structure

- [./prep-data.ipynb](./prep-data.ipynb): Clean and format my raw Telegram chat logs into a useful seq2seq dataset.
- [./train-t5-large.py](./train-t5-large.py): The setup and hparams for my most successful T5-Large run.
- [./train-t5/xl.py](./train-t5/xl.py): The setup and hparams for my most successful T5-XL run.
- [./streamlit-inference.py](./streamlit-inference.py): A simple [Streamlit](https://streamlit.io/) interface that let me load a model, paste a chat history from Telegram, and run inference in realtime. It also optionally allows me to start typing a message that it will attempt to complete (the typeahead use-case).
- [./results.html](https://htmlpreview.github.io/?https://github.com/corbt/autoresponder/blob/main/results.html): A selection of results generated by this model when prompted with real conversations from my family chat.
