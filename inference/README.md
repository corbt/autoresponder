This was an experiment with getting potentially much faster T5 inference using https://github.com/NVIDIA/FasterTransformer. Tests on my hardware with the standard T5 models show a ~5x speedup vs the HuggingFace base.

I'm able to get the FasterTransformer example scripts to run, but they don't actually work right now for my fine-tuned models. The FasterTransformer [t5 examples](https://github.com/NVIDIA/FasterTransformer/tree/main/examples/pytorch/t5) assume a slightly different model architecture than the ones I fine-tuned. For example the activation layers in mine are gated GELUs, instead of RELUs, which means they take an extra set of input weights. It should be relatively simple to update their translation script to accomodate the new model architecture (perhaps as simple as just adding a few lines to map the extra weights [here](https://github.com/NVIDIA/FasterTransformer/blob/main/examples/pytorch/t5/utils/ft_encoder.py#L146)) but this experiment isn't really on the critical path so I'm going to leave it here for now.
